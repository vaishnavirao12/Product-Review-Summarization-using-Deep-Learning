{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"miniproject-se02.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"HzkdHjocylMc","colab_type":"code","outputId":"62f6fa91-d567-48f8-ddea-617c9f63d957","executionInfo":{"status":"ok","timestamp":1553843777281,"user_tz":-330,"elapsed":3478,"user":{"displayName":"Vivek Surya","photoUrl":"https://lh4.googleusercontent.com/-hCHK8rqky1k/AAAAAAAAAAI/AAAAAAAAAHo/fhxI1Q4hMck/s64/photo.jpg","userId":"01032804496414891089"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import re\n","from nltk.corpus import stopwords\n","import time\n","from tensorflow.python.layers.core import Dense\n","from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n","print('TensorFlow Version: {}'.format(tf.__version__))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow Version: 1.13.1\n"],"name":"stdout"}]},{"metadata":{"id":"-qNAAE75lUQ-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"b5fea711-2968-40df-bcdb-4d50bea6407c","executionInfo":{"status":"ok","timestamp":1553848478816,"user_tz":-330,"elapsed":3368,"user":{"displayName":"Vivek Surya","photoUrl":"https://lh4.googleusercontent.com/-hCHK8rqky1k/AAAAAAAAAAI/AAAAAAAAAHo/fhxI1Q4hMck/s64/photo.jpg","userId":"01032804496414891089"}}},"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"execution_count":31,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":31}]},{"metadata":{"id":"dSN7vK6Wy98e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"ebaaa15b-f04a-461b-901e-af4678626740","executionInfo":{"status":"ok","timestamp":1553843813976,"user_tz":-330,"elapsed":40111,"user":{"displayName":"Vivek Surya","photoUrl":"https://lh4.googleusercontent.com/-hCHK8rqky1k/AAAAAAAAAAI/AAAAAAAAAHo/fhxI1Q4hMck/s64/photo.jpg","userId":"01032804496414891089"}}},"cell_type":"code","source":["import pickle\n","from google.colab import drive, files\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"Jle1H2cHkUVf","colab_type":"code","colab":{}},"cell_type":"code","source":["contractions = { \n","\"ain't\": \"am not\",\n","\"aren't\": \"are not\",\n","\"can't\": \"cannot\",\n","\"can't've\": \"cannot have\",\n","\"'cause\": \"because\",\n","\"could've\": \"could have\",\n","\"couldn't\": \"could not\",\n","\"couldn't've\": \"could not have\",\n","\"didn't\": \"did not\",\n","\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\n","\"hadn't\": \"had not\",\n","\"hadn't've\": \"had not have\",\n","\"hasn't\": \"has not\",\n","\"haven't\": \"have not\",\n","\"he'd\": \"he would\",\n","\"he'd've\": \"he would have\",\n","\"he'll\": \"he will\",\n","\"he's\": \"he is\",\n","\"how'd\": \"how did\",\n","\"how'll\": \"how will\",\n","\"how's\": \"how is\",\n","\"i'd\": \"i would\",\n","\"i'll\": \"i will\",\n","\"i'm\": \"i am\",\n","\"i've\": \"i have\",\n","\"isn't\": \"is not\",\n","\"it'd\": \"it would\",\n","\"it'll\": \"it will\",\n","\"it's\": \"it is\",\n","\"let's\": \"let us\",\n","\"ma'am\": \"madam\",\n","\"mayn't\": \"may not\",\n","\"might've\": \"might have\",\n","\"mightn't\": \"might not\",\n","\"must've\": \"must have\",\n","\"mustn't\": \"must not\",\n","\"needn't\": \"need not\",\n","\"oughtn't\": \"ought not\",\n","\"shan't\": \"shall not\",\n","\"sha'n't\": \"shall not\",\n","\"she'd\": \"she would\",\n","\"she'll\": \"she will\",\n","\"she's\": \"she is\",\n","\"should've\": \"should have\",\n","\"shouldn't\": \"should not\",\n","\"that'd\": \"that would\",\n","\"that's\": \"that is\",\n","\"there'd\": \"there had\",\n","\"there's\": \"there is\",\n","\"they'd\": \"they would\",\n","\"they'll\": \"they will\",\n","\"they're\": \"they are\",\n","\"they've\": \"they have\",\n","\"wasn't\": \"was not\",\n","\"we'd\": \"we would\",\n","\"we'll\": \"we will\",\n","\"we're\": \"we are\",\n","\"we've\": \"we have\",\n","\"weren't\": \"were not\",\n","\"what'll\": \"what will\",\n","\"what're\": \"what are\",\n","\"what's\": \"what is\",\n","\"what've\": \"what have\",\n","\"where'd\": \"where did\",\n","\"where's\": \"where is\",\n","\"who'll\": \"who will\",\n","\"who's\": \"who is\",\n","\"won't\": \"will not\",\n","\"wouldn't\": \"would not\",\n","\"you'd\": \"you would\",\n","\"you'll\": \"you will\",\n","\"you're\": \"you are\"\n","}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Gkwf5s0olIB-","colab_type":"code","colab":{}},"cell_type":"code","source":["def clean_text(text, remove_stopwords = True):\n","    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n","    \n","    # Convert words to lower case\n","    text = text.lower()\n","    \n","    # Replace contractions with their longer forms \n","    if True:\n","        # We are not using \"text.split()\" here\n","        #since it is not fool proof, e.g. words followed by punctuations \"Are you kidding?I think you aren't.\"\n","        text = re.findall(r\"[\\w']+\", text)\n","        new_text = []\n","        for word in text:\n","            if word in contractions:\n","                new_text.append(contractions[word])\n","            else:\n","                new_text.append(word)\n","        text = \" \".join(new_text)\n","    \n","    # Format words and remove unwanted characters\n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)# remove links\n","    text = re.sub(r'\\<a href', ' ', text)# remove html link tag\n","    text = re.sub(r'&amp;', '', text) \n","    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n","    text = re.sub(r'<br />', ' ', text)\n","    text = re.sub(r'\\'', ' ', text)\n","    \n","    # Optionally, remove stop words\n","    if remove_stopwords:\n","        text = text.split()\n","        stops = set(stopwords.words(\"english\"))\n","        text = [w for w in text if not w in stops]\n","        text = \" \".join(text)\n","\n","    return text"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l1isBwNry9gy","colab_type":"code","colab":{}},"cell_type":"code","source":["objfile = \"/content/drive/My Drive/ME_SE02_Project/objects.pkl\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"kaApa44Z4muK","colab_type":"code","colab":{}},"cell_type":"code","source":["fp = open(objfile,'rb')\n","objs = pickle.load(fp)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qAue0TpS424m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"f4034ad5-9eb5-4964-e062-ed5cf4d3a8b9","executionInfo":{"status":"ok","timestamp":1553843817935,"user_tz":-330,"elapsed":43983,"user":{"displayName":"Vivek Surya","photoUrl":"https://lh4.googleusercontent.com/-hCHK8rqky1k/AAAAAAAAAAI/AAAAAAAAAHo/fhxI1Q4hMck/s64/photo.jpg","userId":"01032804496414891089"}}},"cell_type":"code","source":["for key,value in objs.items():\n","  print(key)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["sorted_summaries\n","sorted_texts\n","word_embedding_matrix\n","word_counts\n","vocab_to_int\n","int_to_vocab\n"],"name":"stdout"}]},{"metadata":{"id":"wa3rXOQR5IDL","colab_type":"code","colab":{}},"cell_type":"code","source":["sorted_summaries = objs['sorted_summaries']\n","sorted_texts = objs['sorted_texts']\n","word_embedding_matrix = objs['word_embedding_matrix']\n","word_counts = objs['word_counts']\n","vocab_to_int = objs['vocab_to_int']\n","int_to_vocab = objs['int_to_vocab']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bEfJ_PjQTM4R","colab_type":"code","colab":{}},"cell_type":"code","source":["def model_inputs():\n","    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n","    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n","    lr = tf.placeholder(tf.float32, name='learning_rate')\n","    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n","    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n","    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n","    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n","\n","    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K09OgacfyB_I","colab_type":"code","colab":{}},"cell_type":"code","source":["def process_encoding_input(target_data, vocab_to_int, batch_size):  \n","    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\n","    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n","\n","    return dec_input"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vZco4i_xyFIT","colab_type":"code","colab":{}},"cell_type":"code","source":["def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n","    for layer in range(num_layers):\n","        with tf.variable_scope('encoder_{}'.format(layer)):\n","            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n","                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n","            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n","                                                    input_keep_prob = keep_prob)\n","\n","            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n","                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n","            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n","                                                    input_keep_prob = keep_prob)\n","\n","            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n","                                                                    cell_bw, \n","                                                                    rnn_inputs,\n","                                                                    sequence_length,\n","                                                                    dtype=tf.float32)\n","            enc_output = tf.concat(enc_output,2)\n","            # original code is missing this line below, that is how we connect layers \n","            # by feeding the current layer's output to next layer's input\n","            rnn_inputs = enc_output\n","    return enc_output, enc_state"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mRt3oox4yIdN","colab_type":"code","colab":{}},"cell_type":"code","source":["def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\n","                            vocab_size, max_summary_length,batch_size):\n","    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n","                                                        sequence_length=summary_length,\n","                                                        time_major=False)\n","\n","    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n","                                                       helper=training_helper,\n","                                                       initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n","                                                       output_layer = output_layer)\n","\n","    training_logits = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n","                                                           output_time_major=False,\n","                                                           impute_finished=True,\n","                                                           maximum_iterations=max_summary_length)\n","    return training_logits"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E61pm36syLnl","colab_type":"code","colab":{}},"cell_type":"code","source":["def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\n","                             max_summary_length, batch_size):\n","    '''Create the inference logits'''\n","    \n","    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n","    \n","    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n","                                                                start_tokens,\n","                                                                end_token)\n","                \n","    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n","                                                        inference_helper,\n","                                                        dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n","                                                        output_layer)\n","                \n","    inference_logits = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n","                                                            output_time_major=False,\n","                                                            impute_finished=True,\n","                                                            maximum_iterations=max_summary_length)\n","    \n","    return inference_logits"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XvxpOMapyOZG","colab_type":"code","colab":{}},"cell_type":"code","source":["def lstm_cell(lstm_size, keep_prob):\n","    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n","    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n","\n","def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n","                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n","    '''Create the decoding cell and attention for the training and inference decoding layers'''\n","    dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n","    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n","    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n","                                                     enc_output,\n","                                                     text_length,\n","                                                     normalize=False,\n","                                                     name='BahdanauAttention')\n","    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n","    with tf.variable_scope(\"decode\"):\n","        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\n","                                                  output_layer,\n","                                                  vocab_size,\n","                                                  max_summary_length,\n","                                                  batch_size)\n","    with tf.variable_scope(\"decode\", reuse=True):\n","        inference_logits = inference_decoding_layer(embeddings,\n","                                                    vocab_to_int['<GO>'],\n","                                                    vocab_to_int['<EOS>'],\n","                                                    dec_cell,\n","                                                    output_layer,\n","                                                    max_summary_length,\n","                                                    batch_size)\n","    return training_logits, inference_logits\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"njznXa01yRDJ","colab_type":"code","colab":{}},"cell_type":"code","source":["def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n","                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n","    '''Use the previous functions to create the training and inference logits'''\n","    \n","    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n","    embeddings = word_embedding_matrix\n","    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n","    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n","    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\n","    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n","    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n","                                                        embeddings,\n","                                                        enc_output,\n","                                                        enc_state, \n","                                                        vocab_size, \n","                                                        text_length, \n","                                                        summary_length, \n","                                                        max_summary_length,\n","                                                        rnn_size, \n","                                                        vocab_to_int, \n","                                                        keep_prob, \n","                                                        batch_size,\n","                                                        num_layers)\n","    return training_logits, inference_logits"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2GfJpKCjyWDu","colab_type":"code","colab":{}},"cell_type":"code","source":["def pad_sentence_batch(sentence_batch):\n","    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n","    max_sentence = max([len(sentence) for sentence in sentence_batch])\n","    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"881YBz-4yXCV","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_batches(summaries, texts, batch_size):\n","    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n","    for batch_i in range(0, len(texts)//batch_size):\n","        start_i = batch_i * batch_size\n","        summaries_batch = summaries[start_i:start_i + batch_size]\n","        texts_batch = texts[start_i:start_i + batch_size]\n","        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n","        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n","        \n","        # Need the lengths for the _lengths parameters\n","        pad_summaries_lengths = []\n","        for summary in pad_summaries_batch:\n","            pad_summaries_lengths.append(len(summary))\n","        \n","        pad_texts_lengths = []\n","        for text in pad_texts_batch:\n","            pad_texts_lengths.append(len(text))\n","        \n","        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RDOKtIyyyZRD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"11ce2cc6-c2ea-4ed6-bf44-6203387d85b1","executionInfo":{"status":"ok","timestamp":1553843818005,"user_tz":-330,"elapsed":43689,"user":{"displayName":"Vivek Surya","photoUrl":"https://lh4.googleusercontent.com/-hCHK8rqky1k/AAAAAAAAAAI/AAAAAAAAAHo/fhxI1Q4hMck/s64/photo.jpg","userId":"01032804496414891089"}}},"cell_type":"code","source":["print(\"'<PAD>' has id: {}\".format(vocab_to_int['<PAD>']))\n","sorted_summaries_samples = sorted_summaries[7:50]\n","sorted_texts_samples = sorted_texts[7:50]\n","pad_summaries_batch_samples, pad_texts_batch_samples, pad_summaries_lengths_samples, pad_texts_lengths_samples = next(get_batches(\n","    sorted_summaries_samples, sorted_texts_samples, 5))\n","print(\"pad summaries batch samples:\\n\\r {}\".format(pad_summaries_batch_samples))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["'<PAD>' has id: 23639\n","pad summaries batch samples:\n","\r [[   13    28  1055  2730 23639 23639 23639 23639 23639 23639]\n"," [  358    79   379  2173    91  1133   102     0    70   187]\n"," [    1    61   102     4    50    30   232    93   685 23639]\n"," [    0   108 23639 23639 23639 23639 23639 23639 23639 23639]\n"," [    0   102   634 23639 23639 23639 23639 23639 23639 23639]]\n"],"name":"stdout"}]},{"metadata":{"id":"JBo5Jf-Yyb5H","colab_type":"code","colab":{}},"cell_type":"code","source":["# Set the Hyperparameters\n","epochs = 25\n","batch_size = 64\n","rnn_size = 256\n","num_layers = 2\n","learning_rate = 0.005\n","keep_probability = 0.95"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ghJjrO09yet_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":610},"outputId":"83deafd1-e3f7-4b6a-b02e-e03c2ea49dbe","executionInfo":{"status":"ok","timestamp":1553843826083,"user_tz":-330,"elapsed":51712,"user":{"displayName":"Vivek Surya","photoUrl":"https://lh4.googleusercontent.com/-hCHK8rqky1k/AAAAAAAAAAI/AAAAAAAAAHo/fhxI1Q4hMck/s64/photo.jpg","userId":"01032804496414891089"}}},"cell_type":"code","source":["# Build the graph\n","train_graph = tf.Graph()\n","# Set the graph to default to ensure that it is ready for training\n","with train_graph.as_default():\n","    \n","    # Load the model inputs    \n","    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n","\n","    # Create the training and inference logits\n","    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n","                                                      targets, \n","                                                      keep_prob,   \n","                                                      text_length,\n","                                                      summary_length,\n","                                                      max_summary_length,\n","                                                      len(vocab_to_int)+1,\n","                                                      rnn_size, \n","                                                      num_layers, \n","                                                      vocab_to_int,\n","                                                      batch_size)\n","    \n","    # Create tensors for the training logits and inference logits\n","    training_logits = tf.identity(training_logits[0].rnn_output, 'logits')\n","    inference_logits = tf.identity(inference_logits[0].sample_id, name='predictions')\n","    \n","    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n","    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n","\n","    with tf.name_scope(\"optimization\"):\n","        # Loss function\n","        cost = tf.contrib.seq2seq.sequence_loss(\n","            training_logits,\n","            targets,\n","            masks)\n","\n","        # Optimizer\n","        optimizer = tf.train.AdamOptimizer(learning_rate)\n","\n","        # Gradient Clipping\n","        gradients = optimizer.compute_gradients(cost)\n","        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n","        train_op = optimizer.apply_gradients(capped_gradients)\n","print(\"Graph is built.\")\n","graph_location = \"./graph\"\n","print(graph_location)\n","train_writer = tf.summary.FileWriter(graph_location)\n","train_writer.add_graph(train_graph)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-10-db1e10dd2681>:5: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-10-db1e10dd2681>:18: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From <ipython-input-13-e5ed514e649b>:2: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-13-e5ed514e649b>:8: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","Graph is built.\n","./graph\n"],"name":"stdout"}]},{"metadata":{"id":"c_qGr3UOSw07","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"10561a71-c7a0-4545-a030-356cbbf8bb5d","executionInfo":{"status":"ok","timestamp":1553843826092,"user_tz":-330,"elapsed":51697,"user":{"displayName":"Vivek Surya","photoUrl":"https://lh4.googleusercontent.com/-hCHK8rqky1k/AAAAAAAAAAI/AAAAAAAAAHo/fhxI1Q4hMck/s64/photo.jpg","userId":"01032804496414891089"}}},"cell_type":"code","source":["start = 200000\n","end = start + 50000\n","sorted_summaries_short = sorted_summaries[start:end]\n","sorted_texts_short = sorted_texts[start:end]\n","print(\"The shortest text length:\", len(sorted_texts_short[0]))\n","print(\"The longest text length:\",len(sorted_texts_short[-1]))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["The shortest text length: 26\n","The longest text length: 32\n"],"name":"stdout"}]},{"metadata":{"id":"0--cxV_0S2el","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":11944},"outputId":"38d24188-9811-490b-f82b-62cb45c311a2","executionInfo":{"status":"error","timestamp":1553848246851,"user_tz":-330,"elapsed":3174742,"user":{"displayName":"Vivek Surya","photoUrl":"https://lh4.googleusercontent.com/-hCHK8rqky1k/AAAAAAAAAAI/AAAAAAAAAHo/fhxI1Q4hMck/s64/photo.jpg","userId":"01032804496414891089"}}},"cell_type":"code","source":["learning_rate_decay = 0.95\n","min_learning_rate = 0.0005\n","display_step = 20 # Check training loss after every 20 batches\n","stop_early = 0 \n","stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n","per_epoch = 3 # Make 3 update checks per epoch\n","update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n","\n","update_loss = 0 \n","batch_loss = 0\n","summary_update_loss = [] # Record the update losses for saving improvements in the model\n","\n","checkpoint = \"./best_model.ckpt\" \n","with tf.Session(graph=train_graph) as sess:\n","    sess.run(tf.global_variables_initializer())\n","    \n","    # If we want to continue training a previous session\n","    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n","    #loader.restore(sess, checkpoint)\n","    \n","    for epoch_i in range(1, epochs+1):\n","        update_loss = 0\n","        batch_loss = 0\n","        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n","                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n","            start_time = time.time()\n","            _, loss = sess.run(\n","                [train_op, cost],\n","                {input_data: texts_batch,\n","                 targets: summaries_batch,\n","                 lr: learning_rate,\n","                 summary_length: summaries_lengths,\n","                 text_length: texts_lengths,\n","                 keep_prob: keep_probability})\n","\n","            batch_loss += loss\n","            update_loss += loss\n","            end_time = time.time()\n","            batch_time = end_time - start_time\n","\n","            if batch_i % display_step == 0 and batch_i > 0:\n","                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n","                      .format(epoch_i,\n","                              epochs, \n","                              batch_i, \n","                              len(sorted_texts_short) // batch_size, \n","                              batch_loss / display_step, \n","                              batch_time*display_step))\n","                batch_loss = 0\n","\n","            if batch_i % update_check == 0 and batch_i > 0:\n","                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n","                summary_update_loss.append(update_loss)\n","                \n","                # If the update loss is at a new minimum, save the model\n","                if update_loss <= min(summary_update_loss):\n","                    print('New Record!') \n","                    stop_early = 0\n","                    saver = tf.train.Saver() \n","                    saver.save(sess, checkpoint)\n","\n","                else:\n","                    print(\"No Improvement.\")\n","                    stop_early += 1\n","                    if stop_early == stop:\n","                        break\n","                update_loss = 0\n","            \n","                    \n","        # Reduce learning rate, but not below its minimum value\n","        learning_rate *= learning_rate_decay\n","        if learning_rate < min_learning_rate:\n","            learning_rate = min_learning_rate\n","        \n","        if stop_early == stop:\n","            print(\"Stopping Training.\")\n","            break"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Epoch   1/25 Batch   20/781 - Loss:  5.039, Seconds: 5.22\n","Epoch   1/25 Batch   40/781 - Loss:  2.789, Seconds: 5.19\n","Epoch   1/25 Batch   60/781 - Loss:  2.679, Seconds: 5.52\n","Epoch   1/25 Batch   80/781 - Loss:  2.729, Seconds: 5.11\n","Epoch   1/25 Batch  100/781 - Loss:  2.667, Seconds: 5.74\n","Epoch   1/25 Batch  120/781 - Loss:  2.535, Seconds: 5.11\n","Epoch   1/25 Batch  140/781 - Loss:  2.516, Seconds: 5.55\n","Epoch   1/25 Batch  160/781 - Loss:  2.391, Seconds: 5.36\n","Epoch   1/25 Batch  180/781 - Loss:  2.230, Seconds: 5.51\n","Epoch   1/25 Batch  200/781 - Loss:  2.228, Seconds: 5.87\n","Epoch   1/25 Batch  220/781 - Loss:  2.236, Seconds: 5.80\n","Epoch   1/25 Batch  240/781 - Loss:  2.588, Seconds: 5.79\n","Average loss for this update: 2.693\n","New Record!\n","Epoch   1/25 Batch  260/781 - Loss:  2.338, Seconds: 5.83\n","Epoch   1/25 Batch  280/781 - Loss:  2.205, Seconds: 5.94\n","Epoch   1/25 Batch  300/781 - Loss:  2.003, Seconds: 5.68\n","Epoch   1/25 Batch  320/781 - Loss:  1.942, Seconds: 6.11\n","Epoch   1/25 Batch  340/781 - Loss:  1.981, Seconds: 5.71\n","Epoch   1/25 Batch  360/781 - Loss:  2.239, Seconds: 6.00\n","Epoch   1/25 Batch  380/781 - Loss:  2.317, Seconds: 5.79\n","Epoch   1/25 Batch  400/781 - Loss:  2.068, Seconds: 6.09\n","Epoch   1/25 Batch  420/781 - Loss:  2.054, Seconds: 5.74\n","Epoch   1/25 Batch  440/781 - Loss:  1.864, Seconds: 6.06\n","Epoch   1/25 Batch  460/781 - Loss:  1.962, Seconds: 5.59\n","Epoch   1/25 Batch  480/781 - Loss:  1.849, Seconds: 6.06\n","Epoch   1/25 Batch  500/781 - Loss:  2.350, Seconds: 5.91\n","Average loss for this update: 2.077\n","New Record!\n","Epoch   1/25 Batch  520/781 - Loss:  2.187, Seconds: 5.66\n","Epoch   1/25 Batch  540/781 - Loss:  2.177, Seconds: 6.06\n","Epoch   1/25 Batch  560/781 - Loss:  1.858, Seconds: 5.74\n","Epoch   1/25 Batch  580/781 - Loss:  1.843, Seconds: 5.64\n","Epoch   1/25 Batch  600/781 - Loss:  1.668, Seconds: 6.29\n","Epoch   1/25 Batch  620/781 - Loss:  2.304, Seconds: 6.18\n","Epoch   1/25 Batch  640/781 - Loss:  2.171, Seconds: 5.82\n","Epoch   1/25 Batch  660/781 - Loss:  1.950, Seconds: 6.25\n","Epoch   1/25 Batch  680/781 - Loss:  1.699, Seconds: 6.35\n","Epoch   1/25 Batch  700/781 - Loss:  1.824, Seconds: 6.02\n","Epoch   1/25 Batch  720/781 - Loss:  1.857, Seconds: 6.22\n","Epoch   1/25 Batch  740/781 - Loss:  2.215, Seconds: 6.23\n","Epoch   1/25 Batch  760/781 - Loss:  1.951, Seconds: 6.11\n","Average loss for this update: 1.963\n","New Record!\n","Epoch   1/25 Batch  780/781 - Loss:  1.979, Seconds: 6.16\n","Epoch   2/25 Batch   20/781 - Loss:  2.234, Seconds: 5.32\n","Epoch   2/25 Batch   40/781 - Loss:  1.931, Seconds: 5.23\n","Epoch   2/25 Batch   60/781 - Loss:  1.816, Seconds: 5.71\n","Epoch   2/25 Batch   80/781 - Loss:  1.808, Seconds: 5.10\n","Epoch   2/25 Batch  100/781 - Loss:  1.979, Seconds: 5.74\n","Epoch   2/25 Batch  120/781 - Loss:  1.811, Seconds: 5.10\n","Epoch   2/25 Batch  140/781 - Loss:  1.785, Seconds: 5.51\n","Epoch   2/25 Batch  160/781 - Loss:  1.676, Seconds: 5.40\n","Epoch   2/25 Batch  180/781 - Loss:  1.454, Seconds: 5.84\n","Epoch   2/25 Batch  200/781 - Loss:  1.547, Seconds: 5.70\n","Epoch   2/25 Batch  220/781 - Loss:  1.564, Seconds: 5.83\n","Epoch   2/25 Batch  240/781 - Loss:  2.031, Seconds: 5.72\n","Average loss for this update: 1.803\n","New Record!\n","Epoch   2/25 Batch  260/781 - Loss:  1.786, Seconds: 5.82\n","Epoch   2/25 Batch  280/781 - Loss:  1.647, Seconds: 5.91\n","Epoch   2/25 Batch  300/781 - Loss:  1.470, Seconds: 5.74\n","Epoch   2/25 Batch  320/781 - Loss:  1.424, Seconds: 5.54\n","Epoch   2/25 Batch  340/781 - Loss:  1.465, Seconds: 5.50\n","Epoch   2/25 Batch  360/781 - Loss:  1.745, Seconds: 5.82\n","Epoch   2/25 Batch  380/781 - Loss:  1.817, Seconds: 5.86\n","Epoch   2/25 Batch  400/781 - Loss:  1.571, Seconds: 5.93\n","Epoch   2/25 Batch  420/781 - Loss:  1.558, Seconds: 5.94\n","Epoch   2/25 Batch  440/781 - Loss:  1.382, Seconds: 5.86\n","Epoch   2/25 Batch  460/781 - Loss:  1.516, Seconds: 5.57\n","Epoch   2/25 Batch  480/781 - Loss:  1.423, Seconds: 6.07\n","Epoch   2/25 Batch  500/781 - Loss:  1.891, Seconds: 5.75\n","Average loss for this update: 1.585\n","New Record!\n","Epoch   2/25 Batch  520/781 - Loss:  1.714, Seconds: 5.57\n","Epoch   2/25 Batch  540/781 - Loss:  1.695, Seconds: 5.82\n","Epoch   2/25 Batch  560/781 - Loss:  1.372, Seconds: 5.59\n","Epoch   2/25 Batch  580/781 - Loss:  1.426, Seconds: 5.56\n","Epoch   2/25 Batch  600/781 - Loss:  1.311, Seconds: 6.15\n","Epoch   2/25 Batch  620/781 - Loss:  1.859, Seconds: 6.08\n","Epoch   2/25 Batch  640/781 - Loss:  1.726, Seconds: 5.77\n","Epoch   2/25 Batch  660/781 - Loss:  1.522, Seconds: 6.19\n","Epoch   2/25 Batch  680/781 - Loss:  1.316, Seconds: 6.30\n","Epoch   2/25 Batch  700/781 - Loss:  1.462, Seconds: 6.16\n","Epoch   2/25 Batch  720/781 - Loss:  1.505, Seconds: 6.20\n","Epoch   2/25 Batch  740/781 - Loss:  1.863, Seconds: 6.23\n","Epoch   2/25 Batch  760/781 - Loss:  1.608, Seconds: 6.21\n","Average loss for this update: 1.561\n","New Record!\n","Epoch   2/25 Batch  780/781 - Loss:  1.618, Seconds: 6.29\n","Epoch   3/25 Batch   20/781 - Loss:  1.931, Seconds: 5.26\n","Epoch   3/25 Batch   40/781 - Loss:  1.595, Seconds: 5.31\n","Epoch   3/25 Batch   60/781 - Loss:  1.515, Seconds: 5.77\n","Epoch   3/25 Batch   80/781 - Loss:  1.487, Seconds: 5.14\n","Epoch   3/25 Batch  100/781 - Loss:  1.660, Seconds: 5.76\n","Epoch   3/25 Batch  120/781 - Loss:  1.503, Seconds: 5.06\n","Epoch   3/25 Batch  140/781 - Loss:  1.464, Seconds: 5.73\n","Epoch   3/25 Batch  160/781 - Loss:  1.394, Seconds: 5.29\n","Epoch   3/25 Batch  180/781 - Loss:  1.185, Seconds: 5.57\n","Epoch   3/25 Batch  200/781 - Loss:  1.292, Seconds: 5.72\n","Epoch   3/25 Batch  220/781 - Loss:  1.314, Seconds: 5.89\n","Epoch   3/25 Batch  240/781 - Loss:  1.725, Seconds: 5.80\n","Average loss for this update: 1.508\n","New Record!\n","Epoch   3/25 Batch  260/781 - Loss:  1.523, Seconds: 5.67\n","Epoch   3/25 Batch  280/781 - Loss:  1.373, Seconds: 5.90\n","Epoch   3/25 Batch  300/781 - Loss:  1.251, Seconds: 5.67\n","Epoch   3/25 Batch  320/781 - Loss:  1.209, Seconds: 5.55\n","Epoch   3/25 Batch  340/781 - Loss:  1.233, Seconds: 5.75\n","Epoch   3/25 Batch  360/781 - Loss:  1.451, Seconds: 5.94\n","Epoch   3/25 Batch  380/781 - Loss:  1.532, Seconds: 5.81\n","Epoch   3/25 Batch  400/781 - Loss:  1.326, Seconds: 6.02\n","Epoch   3/25 Batch  420/781 - Loss:  1.330, Seconds: 5.98\n","Epoch   3/25 Batch  440/781 - Loss:  1.156, Seconds: 5.88\n","Epoch   3/25 Batch  460/781 - Loss:  1.301, Seconds: 5.39\n","Epoch   3/25 Batch  480/781 - Loss:  1.211, Seconds: 6.02\n","Epoch   3/25 Batch  500/781 - Loss:  1.617, Seconds: 5.78\n","Average loss for this update: 1.342\n","New Record!\n","Epoch   3/25 Batch  520/781 - Loss:  1.477, Seconds: 5.94\n","Epoch   3/25 Batch  540/781 - Loss:  1.452, Seconds: 5.80\n","Epoch   3/25 Batch  560/781 - Loss:  1.153, Seconds: 5.68\n","Epoch   3/25 Batch  580/781 - Loss:  1.239, Seconds: 5.70\n","Epoch   3/25 Batch  600/781 - Loss:  1.147, Seconds: 6.12\n","Epoch   3/25 Batch  620/781 - Loss:  1.626, Seconds: 5.77\n","Epoch   3/25 Batch  640/781 - Loss:  1.489, Seconds: 5.62\n","Epoch   3/25 Batch  660/781 - Loss:  1.326, Seconds: 6.30\n","Epoch   3/25 Batch  680/781 - Loss:  1.137, Seconds: 6.29\n","Epoch   3/25 Batch  700/781 - Loss:  1.276, Seconds: 5.91\n","Epoch   3/25 Batch  720/781 - Loss:  1.302, Seconds: 6.20\n","Epoch   3/25 Batch  740/781 - Loss:  1.641, Seconds: 6.16\n","Epoch   3/25 Batch  760/781 - Loss:  1.416, Seconds: 6.20\n","Average loss for this update: 1.357\n","No Improvement.\n","Epoch   3/25 Batch  780/781 - Loss:  1.426, Seconds: 6.15\n","Epoch   4/25 Batch   20/781 - Loss:  1.723, Seconds: 5.32\n","Epoch   4/25 Batch   40/781 - Loss:  1.384, Seconds: 5.53\n","Epoch   4/25 Batch   60/781 - Loss:  1.358, Seconds: 5.63\n","Epoch   4/25 Batch   80/781 - Loss:  1.307, Seconds: 5.44\n","Epoch   4/25 Batch  100/781 - Loss:  1.458, Seconds: 5.77\n","Epoch   4/25 Batch  120/781 - Loss:  1.312, Seconds: 5.37\n","Epoch   4/25 Batch  140/781 - Loss:  1.281, Seconds: 5.65\n","Epoch   4/25 Batch  160/781 - Loss:  1.238, Seconds: 5.19\n","Epoch   4/25 Batch  180/781 - Loss:  1.046, Seconds: 5.74\n","Epoch   4/25 Batch  200/781 - Loss:  1.163, Seconds: 5.85\n","Epoch   4/25 Batch  220/781 - Loss:  1.156, Seconds: 5.88\n","Epoch   4/25 Batch  240/781 - Loss:  1.526, Seconds: 5.78\n","Average loss for this update: 1.333\n","New Record!\n","Epoch   4/25 Batch  260/781 - Loss:  1.359, Seconds: 5.76\n","Epoch   4/25 Batch  280/781 - Loss:  1.230, Seconds: 5.93\n","Epoch   4/25 Batch  300/781 - Loss:  1.121, Seconds: 5.74\n","Epoch   4/25 Batch  320/781 - Loss:  1.090, Seconds: 5.52\n","Epoch   4/25 Batch  340/781 - Loss:  1.114, Seconds: 6.00\n","Epoch   4/25 Batch  360/781 - Loss:  1.291, Seconds: 5.72\n","Epoch   4/25 Batch  380/781 - Loss:  1.383, Seconds: 6.00\n","Epoch   4/25 Batch  400/781 - Loss:  1.194, Seconds: 5.91\n","Epoch   4/25 Batch  420/781 - Loss:  1.200, Seconds: 5.78\n","Epoch   4/25 Batch  440/781 - Loss:  1.043, Seconds: 5.92\n","Epoch   4/25 Batch  460/781 - Loss:  1.170, Seconds: 5.31\n","Epoch   4/25 Batch  480/781 - Loss:  1.085, Seconds: 6.12\n","Epoch   4/25 Batch  500/781 - Loss:  1.447, Seconds: 5.82\n","Average loss for this update: 1.205\n","New Record!\n","Epoch   4/25 Batch  520/781 - Loss:  1.317, Seconds: 5.68\n","Epoch   4/25 Batch  540/781 - Loss:  1.306, Seconds: 5.75\n","Epoch   4/25 Batch  560/781 - Loss:  1.034, Seconds: 5.58\n","Epoch   4/25 Batch  580/781 - Loss:  1.109, Seconds: 5.75\n","Epoch   4/25 Batch  600/781 - Loss:  1.050, Seconds: 6.12\n","Epoch   4/25 Batch  620/781 - Loss:  1.454, Seconds: 5.84\n","Epoch   4/25 Batch  640/781 - Loss:  1.332, Seconds: 5.84\n","Epoch   4/25 Batch  660/781 - Loss:  1.203, Seconds: 6.29\n","Epoch   4/25 Batch  680/781 - Loss:  1.036, Seconds: 6.24\n","Epoch   4/25 Batch  700/781 - Loss:  1.170, Seconds: 6.09\n","Epoch   4/25 Batch  720/781 - Loss:  1.186, Seconds: 6.13\n","Epoch   4/25 Batch  740/781 - Loss:  1.495, Seconds: 6.19\n","Epoch   4/25 Batch  760/781 - Loss:  1.294, Seconds: 6.21\n","Average loss for this update: 1.23\n","No Improvement.\n","Epoch   4/25 Batch  780/781 - Loss:  1.307, Seconds: 6.16\n","Epoch   5/25 Batch   20/781 - Loss:  1.550, Seconds: 5.30\n","Epoch   5/25 Batch   40/781 - Loss:  1.271, Seconds: 5.25\n","Epoch   5/25 Batch   60/781 - Loss:  1.246, Seconds: 5.71\n","Epoch   5/25 Batch   80/781 - Loss:  1.186, Seconds: 5.16\n","Epoch   5/25 Batch  100/781 - Loss:  1.312, Seconds: 6.06\n","Epoch   5/25 Batch  120/781 - Loss:  1.192, Seconds: 5.14\n","Epoch   5/25 Batch  140/781 - Loss:  1.154, Seconds: 5.44\n","Epoch   5/25 Batch  160/781 - Loss:  1.122, Seconds: 5.18\n","Epoch   5/25 Batch  180/781 - Loss:  0.958, Seconds: 5.49\n","Epoch   5/25 Batch  200/781 - Loss:  1.074, Seconds: 5.81\n","Epoch   5/25 Batch  220/781 - Loss:  1.077, Seconds: 5.89\n","Epoch   5/25 Batch  240/781 - Loss:  1.369, Seconds: 5.79\n","Average loss for this update: 1.213\n","No Improvement.\n","Epoch   5/25 Batch  260/781 - Loss:  1.245, Seconds: 5.85\n","Epoch   5/25 Batch  280/781 - Loss:  1.110, Seconds: 6.14\n","Epoch   5/25 Batch  300/781 - Loss:  1.026, Seconds: 5.82\n","Epoch   5/25 Batch  320/781 - Loss:  0.995, Seconds: 5.60\n","Epoch   5/25 Batch  340/781 - Loss:  1.010, Seconds: 5.81\n","Epoch   5/25 Batch  360/781 - Loss:  1.175, Seconds: 5.83\n","Epoch   5/25 Batch  380/781 - Loss:  1.248, Seconds: 5.70\n","Epoch   5/25 Batch  400/781 - Loss:  1.103, Seconds: 5.99\n","Epoch   5/25 Batch  420/781 - Loss:  1.112, Seconds: 5.89\n","Epoch   5/25 Batch  440/781 - Loss:  0.973, Seconds: 5.77\n","Epoch   5/25 Batch  460/781 - Loss:  1.074, Seconds: 5.32\n","Epoch   5/25 Batch  480/781 - Loss:  0.989, Seconds: 6.22\n","Epoch   5/25 Batch  500/781 - Loss:  1.313, Seconds: 5.88\n","Average loss for this update: 1.101\n","New Record!\n","Epoch   5/25 Batch  520/781 - Loss:  1.213, Seconds: 5.64\n","Epoch   5/25 Batch  540/781 - Loss:  1.194, Seconds: 5.75\n","Epoch   5/25 Batch  560/781 - Loss:  0.965, Seconds: 5.64\n","Epoch   5/25 Batch  580/781 - Loss:  1.038, Seconds: 5.68\n","Epoch   5/25 Batch  600/781 - Loss:  0.971, Seconds: 6.10\n","Epoch   5/25 Batch  620/781 - Loss:  1.333, Seconds: 6.01\n","Epoch   5/25 Batch  640/781 - Loss:  1.233, Seconds: 5.86\n","Epoch   5/25 Batch  660/781 - Loss:  1.126, Seconds: 6.21\n","Epoch   5/25 Batch  680/781 - Loss:  0.967, Seconds: 6.46\n","Epoch   5/25 Batch  700/781 - Loss:  1.089, Seconds: 6.05\n","Epoch   5/25 Batch  720/781 - Loss:  1.090, Seconds: 6.34\n","Epoch   5/25 Batch  740/781 - Loss:  1.377, Seconds: 6.26\n","Epoch   5/25 Batch  760/781 - Loss:  1.206, Seconds: 6.18\n","Average loss for this update: 1.139\n","No Improvement.\n","Epoch   5/25 Batch  780/781 - Loss:  1.207, Seconds: 6.36\n","Epoch   6/25 Batch   20/781 - Loss:  1.432, Seconds: 5.31\n","Epoch   6/25 Batch   40/781 - Loss:  1.153, Seconds: 5.42\n","Epoch   6/25 Batch   60/781 - Loss:  1.163, Seconds: 5.65\n","Epoch   6/25 Batch   80/781 - Loss:  1.119, Seconds: 5.27\n","Epoch   6/25 Batch  100/781 - Loss:  1.212, Seconds: 5.82\n","Epoch   6/25 Batch  120/781 - Loss:  1.101, Seconds: 5.35\n","Epoch   6/25 Batch  140/781 - Loss:  1.078, Seconds: 5.60\n","Epoch   6/25 Batch  160/781 - Loss:  1.043, Seconds: 5.31\n","Epoch   6/25 Batch  180/781 - Loss:  0.882, Seconds: 5.69\n","Epoch   6/25 Batch  200/781 - Loss:  0.992, Seconds: 5.81\n","Epoch   6/25 Batch  220/781 - Loss:  1.006, Seconds: 5.84\n","Epoch   6/25 Batch  240/781 - Loss:  1.254, Seconds: 5.80\n","Average loss for this update: 1.123\n","No Improvement.\n","Epoch   6/25 Batch  260/781 - Loss:  1.146, Seconds: 5.94\n","Epoch   6/25 Batch  280/781 - Loss:  1.028, Seconds: 6.00\n","Epoch   6/25 Batch  300/781 - Loss:  0.966, Seconds: 5.74\n","Epoch   6/25 Batch  320/781 - Loss:  0.940, Seconds: 5.68\n","Epoch   6/25 Batch  340/781 - Loss:  0.949, Seconds: 5.75\n","Epoch   6/25 Batch  360/781 - Loss:  1.090, Seconds: 5.87\n","Epoch   6/25 Batch  380/781 - Loss:  1.151, Seconds: 5.99\n","Epoch   6/25 Batch  400/781 - Loss:  1.015, Seconds: 6.00\n","Epoch   6/25 Batch  420/781 - Loss:  1.035, Seconds: 5.76\n","Epoch   6/25 Batch  440/781 - Loss:  0.904, Seconds: 5.77\n","Epoch   6/25 Batch  460/781 - Loss:  1.010, Seconds: 5.10\n","Epoch   6/25 Batch  480/781 - Loss:  0.925, Seconds: 6.06\n","Epoch   6/25 Batch  500/781 - Loss:  1.203, Seconds: 5.69\n","Average loss for this update: 1.025\n","New Record!\n","Epoch   6/25 Batch  520/781 - Loss:  1.128, Seconds: 5.71\n","Epoch   6/25 Batch  540/781 - Loss:  1.097, Seconds: 5.95\n","Epoch   6/25 Batch  560/781 - Loss:  0.900, Seconds: 5.63\n","Epoch   6/25 Batch  580/781 - Loss:  0.972, Seconds: 5.67\n","Epoch   6/25 Batch  600/781 - Loss:  0.922, Seconds: 6.16\n","Epoch   6/25 Batch  620/781 - Loss:  1.233, Seconds: 6.10\n","Epoch   6/25 Batch  640/781 - Loss:  1.142, Seconds: 5.85\n","Epoch   6/25 Batch  660/781 - Loss:  1.028, Seconds: 6.27\n","Epoch   6/25 Batch  680/781 - Loss:  0.905, Seconds: 6.28\n","Epoch   6/25 Batch  700/781 - Loss:  1.031, Seconds: 5.99\n","Epoch   6/25 Batch  720/781 - Loss:  1.010, Seconds: 6.09\n","Epoch   6/25 Batch  740/781 - Loss:  1.268, Seconds: 6.02\n","Epoch   6/25 Batch  760/781 - Loss:  1.142, Seconds: 6.15\n","Average loss for this update: 1.061\n","No Improvement.\n","Epoch   6/25 Batch  780/781 - Loss:  1.134, Seconds: 6.41\n","Epoch   7/25 Batch   20/781 - Loss:  1.325, Seconds: 5.38\n","Epoch   7/25 Batch   40/781 - Loss:  1.074, Seconds: 5.26\n","Epoch   7/25 Batch   60/781 - Loss:  1.084, Seconds: 5.56\n","Epoch   7/25 Batch   80/781 - Loss:  1.052, Seconds: 5.25\n","Epoch   7/25 Batch  100/781 - Loss:  1.121, Seconds: 5.74\n","Epoch   7/25 Batch  120/781 - Loss:  1.034, Seconds: 5.20\n","Epoch   7/25 Batch  140/781 - Loss:  1.006, Seconds: 5.64\n","Epoch   7/25 Batch  160/781 - Loss:  0.972, Seconds: 5.26\n","Epoch   7/25 Batch  180/781 - Loss:  0.836, Seconds: 5.78\n","Epoch   7/25 Batch  200/781 - Loss:  0.938, Seconds: 5.74\n","Epoch   7/25 Batch  220/781 - Loss:  0.938, Seconds: 5.78\n","Epoch   7/25 Batch  240/781 - Loss:  1.162, Seconds: 5.82\n","Average loss for this update: 1.049\n","No Improvement.\n","Epoch   7/25 Batch  260/781 - Loss:  1.079, Seconds: 6.00\n","Epoch   7/25 Batch  280/781 - Loss:  0.962, Seconds: 5.94\n","Epoch   7/25 Batch  300/781 - Loss:  0.903, Seconds: 5.79\n","Epoch   7/25 Batch  320/781 - Loss:  0.886, Seconds: 5.57\n","Epoch   7/25 Batch  340/781 - Loss:  0.891, Seconds: 5.67\n","Epoch   7/25 Batch  360/781 - Loss:  1.008, Seconds: 5.90\n","Epoch   7/25 Batch  380/781 - Loss:  1.061, Seconds: 5.85\n","Epoch   7/25 Batch  400/781 - Loss:  0.940, Seconds: 6.11\n","Epoch   7/25 Batch  420/781 - Loss:  0.999, Seconds: 5.86\n","Epoch   7/25 Batch  440/781 - Loss:  0.859, Seconds: 5.94\n","Epoch   7/25 Batch  460/781 - Loss:  0.951, Seconds: 5.45\n","Epoch   7/25 Batch  480/781 - Loss:  0.869, Seconds: 6.30\n","Epoch   7/25 Batch  500/781 - Loss:  1.138, Seconds: 5.78\n","Average loss for this update: 0.963\n","New Record!\n","Epoch   7/25 Batch  520/781 - Loss:  1.065, Seconds: 5.67\n","Epoch   7/25 Batch  540/781 - Loss:  1.024, Seconds: 5.88\n","Epoch   7/25 Batch  560/781 - Loss:  0.853, Seconds: 5.78\n","Epoch   7/25 Batch  580/781 - Loss:  0.923, Seconds: 5.60\n","Epoch   7/25 Batch  600/781 - Loss:  0.876, Seconds: 6.15\n","Epoch   7/25 Batch  620/781 - Loss:  1.175, Seconds: 5.86\n","Epoch   7/25 Batch  640/781 - Loss:  1.069, Seconds: 5.85\n","Epoch   7/25 Batch  660/781 - Loss:  0.968, Seconds: 6.18\n","Epoch   7/25 Batch  680/781 - Loss:  0.855, Seconds: 6.19\n","Epoch   7/25 Batch  700/781 - Loss:  0.966, Seconds: 5.94\n","Epoch   7/25 Batch  720/781 - Loss:  0.972, Seconds: 6.21\n","Epoch   7/25 Batch  740/781 - Loss:  1.199, Seconds: 6.05\n","Epoch   7/25 Batch  760/781 - Loss:  1.087, Seconds: 6.08\n","Average loss for this update: 1.004\n","No Improvement.\n","Epoch   7/25 Batch  780/781 - Loss:  1.081, Seconds: 6.50\n","Epoch   8/25 Batch   20/781 - Loss:  1.245, Seconds: 5.32\n","Epoch   8/25 Batch   40/781 - Loss:  1.031, Seconds: 5.34\n","Epoch   8/25 Batch   60/781 - Loss:  1.031, Seconds: 5.71\n","Epoch   8/25 Batch   80/781 - Loss:  0.992, Seconds: 5.21\n","Epoch   8/25 Batch  100/781 - Loss:  1.059, Seconds: 5.72\n","Epoch   8/25 Batch  120/781 - Loss:  0.975, Seconds: 5.03\n","Epoch   8/25 Batch  140/781 - Loss:  0.957, Seconds: 5.55\n","Epoch   8/25 Batch  160/781 - Loss:  0.933, Seconds: 5.25\n","Epoch   8/25 Batch  180/781 - Loss:  0.791, Seconds: 5.73\n","Epoch   8/25 Batch  200/781 - Loss:  0.894, Seconds: 5.81\n","Epoch   8/25 Batch  220/781 - Loss:  0.888, Seconds: 5.93\n","Epoch   8/25 Batch  240/781 - Loss:  1.100, Seconds: 5.56\n","Average loss for this update: 0.995\n","No Improvement.\n","Epoch   8/25 Batch  260/781 - Loss:  1.026, Seconds: 5.91\n","Epoch   8/25 Batch  280/781 - Loss:  0.919, Seconds: 5.80\n","Epoch   8/25 Batch  300/781 - Loss:  0.866, Seconds: 5.73\n","Epoch   8/25 Batch  320/781 - Loss:  0.838, Seconds: 5.58\n","Epoch   8/25 Batch  340/781 - Loss:  0.856, Seconds: 5.74\n","Epoch   8/25 Batch  360/781 - Loss:  0.959, Seconds: 5.88\n","Epoch   8/25 Batch  380/781 - Loss:  1.004, Seconds: 5.78\n","Epoch   8/25 Batch  400/781 - Loss:  0.894, Seconds: 6.24\n","Epoch   8/25 Batch  420/781 - Loss:  0.940, Seconds: 5.84\n","Epoch   8/25 Batch  440/781 - Loss:  0.818, Seconds: 5.80\n","Epoch   8/25 Batch  460/781 - Loss:  0.901, Seconds: 5.53\n","Epoch   8/25 Batch  480/781 - Loss:  0.827, Seconds: 6.17\n","Epoch   8/25 Batch  500/781 - Loss:  1.071, Seconds: 5.84\n","Average loss for this update: 0.913\n","New Record!\n","Epoch   8/25 Batch  520/781 - Loss:  0.989, Seconds: 5.82\n","Epoch   8/25 Batch  540/781 - Loss:  0.964, Seconds: 5.84\n","Epoch   8/25 Batch  560/781 - Loss:  0.797, Seconds: 5.74\n","Epoch   8/25 Batch  580/781 - Loss:  0.856, Seconds: 5.65\n","Epoch   8/25 Batch  600/781 - Loss:  0.841, Seconds: 6.10\n","Epoch   8/25 Batch  620/781 - Loss:  1.082, Seconds: 5.89\n","Epoch   8/25 Batch  640/781 - Loss:  1.005, Seconds: 5.87\n","Epoch   8/25 Batch  660/781 - Loss:  0.920, Seconds: 6.37\n","Epoch   8/25 Batch  680/781 - Loss:  0.818, Seconds: 6.45\n","Epoch   8/25 Batch  700/781 - Loss:  0.917, Seconds: 5.73\n","Epoch   8/25 Batch  720/781 - Loss:  0.893, Seconds: 6.24\n","Epoch   8/25 Batch  740/781 - Loss:  1.118, Seconds: 6.19\n","Epoch   8/25 Batch  760/781 - Loss:  1.026, Seconds: 6.23\n","Average loss for this update: 0.943\n","No Improvement.\n","Epoch   8/25 Batch  780/781 - Loss:  1.030, Seconds: 6.20\n","Epoch   9/25 Batch   20/781 - Loss:  1.177, Seconds: 5.28\n","Epoch   9/25 Batch   40/781 - Loss:  0.982, Seconds: 5.35\n","Epoch   9/25 Batch   60/781 - Loss:  0.987, Seconds: 5.55\n","Epoch   9/25 Batch   80/781 - Loss:  0.944, Seconds: 5.33\n","Epoch   9/25 Batch  100/781 - Loss:  1.003, Seconds: 5.84\n","Epoch   9/25 Batch  120/781 - Loss:  0.927, Seconds: 5.10\n","Epoch   9/25 Batch  140/781 - Loss:  0.913, Seconds: 5.71\n","Epoch   9/25 Batch  160/781 - Loss:  0.891, Seconds: 5.24\n","Epoch   9/25 Batch  180/781 - Loss:  0.748, Seconds: 5.60\n","Epoch   9/25 Batch  200/781 - Loss:  0.856, Seconds: 5.92\n","Epoch   9/25 Batch  220/781 - Loss:  0.845, Seconds: 5.85\n","Epoch   9/25 Batch  240/781 - Loss:  1.027, Seconds: 5.69\n","Average loss for this update: 0.945\n","No Improvement.\n","Epoch   9/25 Batch  260/781 - Loss:  0.971, Seconds: 5.90\n","Epoch   9/25 Batch  280/781 - Loss:  0.876, Seconds: 5.73\n","Epoch   9/25 Batch  300/781 - Loss:  0.832, Seconds: 5.95\n","Epoch   9/25 Batch  320/781 - Loss:  0.815, Seconds: 5.53\n","Epoch   9/25 Batch  340/781 - Loss:  0.823, Seconds: 6.04\n","Epoch   9/25 Batch  360/781 - Loss:  0.910, Seconds: 5.90\n","Epoch   9/25 Batch  380/781 - Loss:  0.953, Seconds: 6.08\n","Epoch   9/25 Batch  400/781 - Loss:  0.854, Seconds: 6.05\n","Epoch   9/25 Batch  420/781 - Loss:  0.900, Seconds: 5.87\n","Epoch   9/25 Batch  440/781 - Loss:  0.784, Seconds: 5.88\n","Epoch   9/25 Batch  460/781 - Loss:  0.865, Seconds: 5.45\n","Epoch   9/25 Batch  480/781 - Loss:  0.786, Seconds: 6.21\n","Epoch   9/25 Batch  500/781 - Loss:  1.021, Seconds: 6.13\n","Average loss for this update: 0.874\n","New Record!\n","Epoch   9/25 Batch  520/781 - Loss:  0.955, Seconds: 6.69\n","Epoch   9/25 Batch  540/781 - Loss:  0.924, Seconds: 5.79\n","Epoch   9/25 Batch  560/781 - Loss:  0.774, Seconds: 5.80\n","Epoch   9/25 Batch  580/781 - Loss:  0.840, Seconds: 5.71\n","Epoch   9/25 Batch  600/781 - Loss:  0.809, Seconds: 6.18\n","Epoch   9/25 Batch  620/781 - Loss:  1.042, Seconds: 6.02\n","Epoch   9/25 Batch  640/781 - Loss:  0.975, Seconds: 5.80\n","Epoch   9/25 Batch  660/781 - Loss:  0.897, Seconds: 6.52\n","Epoch   9/25 Batch  680/781 - Loss:  0.780, Seconds: 6.53\n","Epoch   9/25 Batch  700/781 - Loss:  0.891, Seconds: 5.88\n","Epoch   9/25 Batch  720/781 - Loss:  0.885, Seconds: 6.20\n","Epoch   9/25 Batch  740/781 - Loss:  1.081, Seconds: 6.13\n","Epoch   9/25 Batch  760/781 - Loss:  0.984, Seconds: 6.17\n","Average loss for this update: 0.914\n","No Improvement.\n","Epoch   9/25 Batch  780/781 - Loss:  0.994, Seconds: 6.29\n","Epoch  10/25 Batch   20/781 - Loss:  1.134, Seconds: 5.48\n","Epoch  10/25 Batch   40/781 - Loss:  0.945, Seconds: 5.40\n","Epoch  10/25 Batch   60/781 - Loss:  0.949, Seconds: 5.55\n","Epoch  10/25 Batch   80/781 - Loss:  0.909, Seconds: 5.11\n","Epoch  10/25 Batch  100/781 - Loss:  0.955, Seconds: 5.75\n","Epoch  10/25 Batch  120/781 - Loss:  0.885, Seconds: 5.24\n","Epoch  10/25 Batch  140/781 - Loss:  0.872, Seconds: 5.59\n","Epoch  10/25 Batch  160/781 - Loss:  0.858, Seconds: 5.35\n","Epoch  10/25 Batch  180/781 - Loss:  0.737, Seconds: 5.67\n","Epoch  10/25 Batch  200/781 - Loss:  0.831, Seconds: 5.81\n","Epoch  10/25 Batch  220/781 - Loss:  0.818, Seconds: 5.92\n","Epoch  10/25 Batch  240/781 - Loss:  0.987, Seconds: 5.72\n","Average loss for this update: 0.91\n","No Improvement.\n","Epoch  10/25 Batch  260/781 - Loss:  0.944, Seconds: 5.86\n","Epoch  10/25 Batch  280/781 - Loss:  0.848, Seconds: 5.84\n","Epoch  10/25 Batch  300/781 - Loss:  0.811, Seconds: 6.01\n","Epoch  10/25 Batch  320/781 - Loss:  0.776, Seconds: 5.57\n","Epoch  10/25 Batch  340/781 - Loss:  0.806, Seconds: 5.75\n","Epoch  10/25 Batch  360/781 - Loss:  0.877, Seconds: 5.72\n","Epoch  10/25 Batch  380/781 - Loss:  0.922, Seconds: 5.87\n","Epoch  10/25 Batch  400/781 - Loss:  0.830, Seconds: 5.89\n","Epoch  10/25 Batch  420/781 - Loss:  0.852, Seconds: 5.77\n","Epoch  10/25 Batch  440/781 - Loss:  0.765, Seconds: 5.84\n","Epoch  10/25 Batch  460/781 - Loss:  0.832, Seconds: 5.34\n","Epoch  10/25 Batch  480/781 - Loss:  0.766, Seconds: 6.13\n","Epoch  10/25 Batch  500/781 - Loss:  0.976, Seconds: 5.78\n","Average loss for this update: 0.843\n","New Record!\n","Epoch  10/25 Batch  520/781 - Loss:  0.923, Seconds: 5.65\n","Epoch  10/25 Batch  540/781 - Loss:  0.894, Seconds: 5.71\n","Epoch  10/25 Batch  560/781 - Loss:  0.742, Seconds: 5.51\n","Epoch  10/25 Batch  580/781 - Loss:  0.801, Seconds: 5.62\n","Epoch  10/25 Batch  600/781 - Loss:  0.783, Seconds: 6.05\n","Epoch  10/25 Batch  620/781 - Loss:  0.993, Seconds: 6.15\n","Epoch  10/25 Batch  640/781 - Loss:  0.935, Seconds: 5.80\n","Epoch  10/25 Batch  660/781 - Loss:  0.861, Seconds: 6.33\n","Epoch  10/25 Batch  680/781 - Loss:  0.754, Seconds: 6.36\n","Epoch  10/25 Batch  700/781 - Loss:  0.864, Seconds: 5.85\n","Epoch  10/25 Batch  720/781 - Loss:  0.847, Seconds: 6.26\n","Epoch  10/25 Batch  740/781 - Loss:  1.024, Seconds: 6.47\n","Epoch  10/25 Batch  760/781 - Loss:  0.944, Seconds: 6.20\n","Average loss for this update: 0.876\n","No Improvement.\n","Epoch  10/25 Batch  780/781 - Loss:  0.944, Seconds: 6.29\n","Epoch  11/25 Batch   20/781 - Loss:  1.090, Seconds: 5.20\n","Epoch  11/25 Batch   40/781 - Loss:  0.908, Seconds: 5.26\n","Epoch  11/25 Batch   60/781 - Loss:  0.910, Seconds: 5.56\n","Epoch  11/25 Batch   80/781 - Loss:  0.891, Seconds: 5.19\n","Epoch  11/25 Batch  100/781 - Loss:  0.922, Seconds: 5.83\n","Epoch  11/25 Batch  120/781 - Loss:  0.844, Seconds: 5.04\n","Epoch  11/25 Batch  140/781 - Loss:  0.841, Seconds: 5.57\n","Epoch  11/25 Batch  160/781 - Loss:  0.828, Seconds: 5.28\n","Epoch  11/25 Batch  180/781 - Loss:  0.703, Seconds: 5.44\n","Epoch  11/25 Batch  200/781 - Loss:  0.804, Seconds: 5.66\n","Epoch  11/25 Batch  220/781 - Loss:  0.800, Seconds: 5.94\n","Epoch  11/25 Batch  240/781 - Loss:  0.952, Seconds: 5.73\n","Average loss for this update: 0.877\n","No Improvement.\n","Epoch  11/25 Batch  260/781 - Loss:  0.894, Seconds: 6.07\n","Epoch  11/25 Batch  280/781 - Loss:  0.816, Seconds: 6.00\n","Epoch  11/25 Batch  300/781 - Loss:  0.782, Seconds: 5.81\n","Epoch  11/25 Batch  320/781 - Loss:  0.757, Seconds: 5.48\n","Epoch  11/25 Batch  340/781 - Loss:  0.767, Seconds: 5.66\n","Epoch  11/25 Batch  360/781 - Loss:  0.861, Seconds: 5.86\n","Epoch  11/25 Batch  380/781 - Loss:  0.891, Seconds: 5.83\n","Epoch  11/25 Batch  400/781 - Loss:  0.809, Seconds: 6.00\n","Epoch  11/25 Batch  420/781 - Loss:  0.846, Seconds: 5.87\n","Epoch  11/25 Batch  440/781 - Loss:  0.738, Seconds: 5.85\n","Epoch  11/25 Batch  460/781 - Loss:  0.813, Seconds: 5.35\n","Epoch  11/25 Batch  480/781 - Loss:  0.738, Seconds: 6.12\n","Epoch  11/25 Batch  500/781 - Loss:  0.938, Seconds: 5.96\n","Average loss for this update: 0.817\n","New Record!\n","Epoch  11/25 Batch  520/781 - Loss:  0.884, Seconds: 5.80\n","Epoch  11/25 Batch  540/781 - Loss:  0.862, Seconds: 5.79\n","Epoch  11/25 Batch  560/781 - Loss:  0.717, Seconds: 5.63\n","Epoch  11/25 Batch  580/781 - Loss:  0.776, Seconds: 5.70\n","Epoch  11/25 Batch  600/781 - Loss:  0.755, Seconds: 6.37\n","Epoch  11/25 Batch  620/781 - Loss:  0.960, Seconds: 6.24\n","Epoch  11/25 Batch  640/781 - Loss:  0.889, Seconds: 5.74\n","Epoch  11/25 Batch  660/781 - Loss:  0.831, Seconds: 6.33\n","Epoch  11/25 Batch  680/781 - Loss:  0.722, Seconds: 6.43\n","Epoch  11/25 Batch  700/781 - Loss:  0.816, Seconds: 6.00\n","Epoch  11/25 Batch  720/781 - Loss:  0.815, Seconds: 6.30\n","Epoch  11/25 Batch  740/781 - Loss:  0.998, Seconds: 6.19\n","Epoch  11/25 Batch  760/781 - Loss:  0.912, Seconds: 6.12\n","Average loss for this update: 0.844\n","No Improvement.\n","Epoch  11/25 Batch  780/781 - Loss:  0.919, Seconds: 6.29\n","Epoch  12/25 Batch   20/781 - Loss:  1.038, Seconds: 5.16\n","Epoch  12/25 Batch   40/781 - Loss:  0.859, Seconds: 5.31\n","Epoch  12/25 Batch   60/781 - Loss:  0.885, Seconds: 5.63\n","Epoch  12/25 Batch   80/781 - Loss:  0.848, Seconds: 5.18\n","Epoch  12/25 Batch  100/781 - Loss:  0.892, Seconds: 5.84\n","Epoch  12/25 Batch  120/781 - Loss:  0.825, Seconds: 5.06\n","Epoch  12/25 Batch  140/781 - Loss:  0.813, Seconds: 5.67\n","Epoch  12/25 Batch  160/781 - Loss:  0.801, Seconds: 5.26\n","Epoch  12/25 Batch  180/781 - Loss:  0.685, Seconds: 5.65\n","Epoch  12/25 Batch  200/781 - Loss:  0.786, Seconds: 5.72\n","Epoch  12/25 Batch  220/781 - Loss:  0.764, Seconds: 5.82\n","Epoch  12/25 Batch  240/781 - Loss:  0.919, Seconds: 5.75\n","Average loss for this update: 0.845\n","No Improvement.\n","Epoch  12/25 Batch  260/781 - Loss:  0.866, Seconds: 6.08\n","Epoch  12/25 Batch  280/781 - Loss:  0.793, Seconds: 5.88\n","Epoch  12/25 Batch  300/781 - Loss:  0.756, Seconds: 5.86\n","Epoch  12/25 Batch  320/781 - Loss:  0.735, Seconds: 5.51\n","Epoch  12/25 Batch  340/781 - Loss:  0.750, Seconds: 5.86\n","Epoch  12/25 Batch  360/781 - Loss:  0.815, Seconds: 5.77\n","Epoch  12/25 Batch  380/781 - Loss:  0.862, Seconds: 6.05\n","Epoch  12/25 Batch  400/781 - Loss:  0.773, Seconds: 6.09\n","Epoch  12/25 Batch  420/781 - Loss:  0.815, Seconds: 5.87\n","Epoch  12/25 Batch  440/781 - Loss:  0.721, Seconds: 6.13\n","Epoch  12/25 Batch  460/781 - Loss:  0.796, Seconds: 5.38\n","Epoch  12/25 Batch  480/781 - Loss:  0.726, Seconds: 6.16\n","Epoch  12/25 Batch  500/781 - Loss:  0.915, Seconds: 6.05\n","Average loss for this update: 0.792\n","New Record!\n","Epoch  12/25 Batch  520/781 - Loss:  0.850, Seconds: 5.64\n","Epoch  12/25 Batch  540/781 - Loss:  0.845, Seconds: 5.95\n","Epoch  12/25 Batch  560/781 - Loss:  0.704, Seconds: 5.61\n","Epoch  12/25 Batch  580/781 - Loss:  0.771, Seconds: 5.66\n","Epoch  12/25 Batch  600/781 - Loss:  0.740, Seconds: 6.13\n","Epoch  12/25 Batch  620/781 - Loss:  0.931, Seconds: 5.99\n","Epoch  12/25 Batch  640/781 - Loss:  0.877, Seconds: 5.72\n","Epoch  12/25 Batch  660/781 - Loss:  0.805, Seconds: 6.20\n","Epoch  12/25 Batch  680/781 - Loss:  0.698, Seconds: 6.44\n","Epoch  12/25 Batch  700/781 - Loss:  0.805, Seconds: 5.96\n","Epoch  12/25 Batch  720/781 - Loss:  0.789, Seconds: 6.52\n","Epoch  12/25 Batch  740/781 - Loss:  0.969, Seconds: 6.40\n","Epoch  12/25 Batch  760/781 - Loss:  0.899, Seconds: 6.39\n","Average loss for this update: 0.826\n","No Improvement.\n","Epoch  12/25 Batch  780/781 - Loss:  0.916, Seconds: 6.34\n","Epoch  13/25 Batch   20/781 - Loss:  1.019, Seconds: 5.40\n","Epoch  13/25 Batch   40/781 - Loss:  0.851, Seconds: 5.25\n","Epoch  13/25 Batch   60/781 - Loss:  0.861, Seconds: 5.58\n","Epoch  13/25 Batch   80/781 - Loss:  0.841, Seconds: 5.24\n","Epoch  13/25 Batch  100/781 - Loss:  0.863, Seconds: 5.70\n","Epoch  13/25 Batch  120/781 - Loss:  0.805, Seconds: 5.22\n","Epoch  13/25 Batch  140/781 - Loss:  0.795, Seconds: 5.71\n","Epoch  13/25 Batch  160/781 - Loss:  0.779, Seconds: 5.24\n","Epoch  13/25 Batch  180/781 - Loss:  0.669, Seconds: 5.70\n","Epoch  13/25 Batch  200/781 - Loss:  0.770, Seconds: 6.00\n","Epoch  13/25 Batch  220/781 - Loss:  0.759, Seconds: 5.91\n","Epoch  13/25 Batch  240/781 - Loss:  0.895, Seconds: 5.64\n","Average loss for this update: 0.828\n","No Improvement.\n","Epoch  13/25 Batch  260/781 - Loss:  0.846, Seconds: 5.91\n","Epoch  13/25 Batch  280/781 - Loss:  0.770, Seconds: 5.87\n","Epoch  13/25 Batch  300/781 - Loss:  0.743, Seconds: 5.78\n","Epoch  13/25 Batch  320/781 - Loss:  0.735, Seconds: 5.54\n","Epoch  13/25 Batch  340/781 - Loss:  0.735, Seconds: 5.78\n","Epoch  13/25 Batch  360/781 - Loss:  0.804, Seconds: 5.67\n","Epoch  13/25 Batch  380/781 - Loss:  0.849, Seconds: 5.69\n","Epoch  13/25 Batch  400/781 - Loss:  0.751, Seconds: 6.01\n","Epoch  13/25 Batch  420/781 - Loss:  0.788, Seconds: 5.82\n","Epoch  13/25 Batch  440/781 - Loss:  0.690, Seconds: 5.85\n","Epoch  13/25 Batch  460/781 - Loss:  0.756, Seconds: 5.44\n","Epoch  13/25 Batch  480/781 - Loss:  0.704, Seconds: 6.11\n","Epoch  13/25 Batch  500/781 - Loss:  0.888, Seconds: 6.08\n","Average loss for this update: 0.771\n","New Record!\n","Epoch  13/25 Batch  520/781 - Loss:  0.824, Seconds: 5.66\n","Epoch  13/25 Batch  540/781 - Loss:  0.810, Seconds: 5.70\n","Epoch  13/25 Batch  560/781 - Loss:  0.678, Seconds: 5.58\n","Epoch  13/25 Batch  580/781 - Loss:  0.751, Seconds: 5.52\n","Epoch  13/25 Batch  600/781 - Loss:  0.724, Seconds: 6.24\n","Epoch  13/25 Batch  620/781 - Loss:  0.898, Seconds: 5.97\n","Epoch  13/25 Batch  640/781 - Loss:  0.854, Seconds: 5.75\n","Epoch  13/25 Batch  660/781 - Loss:  0.781, Seconds: 6.31\n","Epoch  13/25 Batch  680/781 - Loss:  0.693, Seconds: 6.49\n","Epoch  13/25 Batch  700/781 - Loss:  0.786, Seconds: 5.88\n","Epoch  13/25 Batch  720/781 - Loss:  0.756, Seconds: 6.26\n","Epoch  13/25 Batch  740/781 - Loss:  0.942, Seconds: 6.31\n","Epoch  13/25 Batch  760/781 - Loss:  0.871, Seconds: 6.28\n","Average loss for this update: 0.802\n","No Improvement.\n","Epoch  13/25 Batch  780/781 - Loss:  0.887, Seconds: 6.42\n","Epoch  14/25 Batch   20/781 - Loss:  0.978, Seconds: 5.32\n","Epoch  14/25 Batch   40/781 - Loss:  0.829, Seconds: 5.29\n","Epoch  14/25 Batch   60/781 - Loss:  0.853, Seconds: 5.85\n","Epoch  14/25 Batch   80/781 - Loss:  0.824, Seconds: 5.08\n","Epoch  14/25 Batch  100/781 - Loss:  0.845, Seconds: 5.89\n","Epoch  14/25 Batch  120/781 - Loss:  0.773, Seconds: 5.29\n","Epoch  14/25 Batch  140/781 - Loss:  0.775, Seconds: 5.61\n","Epoch  14/25 Batch  160/781 - Loss:  0.761, Seconds: 5.32\n","Epoch  14/25 Batch  180/781 - Loss:  0.665, Seconds: 5.48\n","Epoch  14/25 Batch  200/781 - Loss:  0.740, Seconds: 5.76\n","Epoch  14/25 Batch  220/781 - Loss:  0.747, Seconds: 5.86\n","Epoch  14/25 Batch  240/781 - Loss:  0.873, Seconds: 5.82\n","Average loss for this update: 0.809\n","No Improvement.\n","Epoch  14/25 Batch  260/781 - Loss:  0.846, Seconds: 5.78\n","Epoch  14/25 Batch  280/781 - Loss:  0.751, Seconds: 5.87\n","Epoch  14/25 Batch  300/781 - Loss:  0.726, Seconds: 5.68\n","Epoch  14/25 Batch  320/781 - Loss:  0.723, Seconds: 5.61\n","Epoch  14/25 Batch  340/781 - Loss:  0.718, Seconds: 5.63\n","Epoch  14/25 Batch  360/781 - Loss:  0.783, Seconds: 5.77\n","Epoch  14/25 Batch  380/781 - Loss:  0.820, Seconds: 5.88\n","Epoch  14/25 Batch  400/781 - Loss:  0.753, Seconds: 6.21\n","Epoch  14/25 Batch  420/781 - Loss:  0.787, Seconds: 5.85\n","Epoch  14/25 Batch  440/781 - Loss:  0.688, Seconds: 5.79\n","Epoch  14/25 Batch  460/781 - Loss:  0.754, Seconds: 5.64\n","Epoch  14/25 Batch  480/781 - Loss:  0.690, Seconds: 6.23\n","Epoch  14/25 Batch  500/781 - Loss:  0.873, Seconds: 5.89\n","Average loss for this update: 0.759\n","New Record!\n","Epoch  14/25 Batch  520/781 - Loss:  0.810, Seconds: 5.86\n","Epoch  14/25 Batch  540/781 - Loss:  0.787, Seconds: 5.75\n","Epoch  14/25 Batch  560/781 - Loss:  0.665, Seconds: 5.72\n","Epoch  14/25 Batch  580/781 - Loss:  0.727, Seconds: 5.72\n","Epoch  14/25 Batch  600/781 - Loss:  0.705, Seconds: 6.08\n","Epoch  14/25 Batch  620/781 - Loss:  0.892, Seconds: 5.84\n","Epoch  14/25 Batch  640/781 - Loss:  0.834, Seconds: 5.85\n","Epoch  14/25 Batch  660/781 - Loss:  0.759, Seconds: 6.06\n","Epoch  14/25 Batch  680/781 - Loss:  0.686, Seconds: 6.24\n","Epoch  14/25 Batch  700/781 - Loss:  0.773, Seconds: 5.91\n","Epoch  14/25 Batch  720/781 - Loss:  0.745, Seconds: 6.25\n","Epoch  14/25 Batch  740/781 - Loss:  0.924, Seconds: 6.16\n","Epoch  14/25 Batch  760/781 - Loss:  0.843, Seconds: 6.25\n","Average loss for this update: 0.785\n","No Improvement.\n","Epoch  14/25 Batch  780/781 - Loss:  0.860, Seconds: 6.24\n","Epoch  15/25 Batch   20/781 - Loss:  0.952, Seconds: 5.39\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-5b7b99d7ff0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                  \u001b[0msummary_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msummaries_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                  \u001b[0mtext_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtexts_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                  keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"C83v-2PjS7X1","colab_type":"code","colab":{}},"cell_type":"code","source":["def text_to_seq(text):\n","    '''Prepare the text for the model'''\n","    \n","    text = clean_text(text)\n","    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vAWumuXmkQHz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":367},"outputId":"1e8d4422-f5a6-474e-9b81-8337ad17a2af","executionInfo":{"status":"ok","timestamp":1553848629601,"user_tz":-330,"elapsed":7469,"user":{"displayName":"Vivek Surya","photoUrl":"https://lh4.googleusercontent.com/-hCHK8rqky1k/AAAAAAAAAAI/AAAAAAAAAHo/fhxI1Q4hMck/s64/photo.jpg","userId":"01032804496414891089"}}},"cell_type":"code","source":["input_sentences=[\"The coffee tasted great and was at such a good price! I highly recommend this to everyone!\",\n","                 \"love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets\"]\n","generagte_summary_length =  [3,2]\n","\n","texts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\n","checkpoint = \"./best_model.ckpt\"\n","if type(generagte_summary_length) is list:\n","    if len(input_sentences)!=len(generagte_summary_length):\n","        raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\n","    generagte_summary_length_list = generagte_summary_length\n","else:\n","    generagte_summary_length_list = [generagte_summary_length] * len(texts)\n","loaded_graph = tf.Graph()\n","with tf.Session(graph=loaded_graph) as sess:\n","    # Load saved model\n","    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n","    loader.restore(sess, checkpoint)\n","    input_data = loaded_graph.get_tensor_by_name('input:0')\n","    logits = loaded_graph.get_tensor_by_name('predictions:0')\n","    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n","    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n","    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n","    #Multiply by batch_size to match the model's input parameters\n","    for i, text in enumerate(texts):\n","        generagte_summary_length = generagte_summary_length_list[i]\n","        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n","                                          summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \n","                                          text_length: [len(text)]*batch_size,\n","                                          keep_prob: 1.0})[0] \n","        # Remove the padding from the summaries\n","        pad = vocab_to_int[\"<PAD>\"] \n","        print('\\n- Review:\\n\\r {}\\n'.format(input_sentences[i]))\n","        print('\\n- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n","\n","- Review:\n"," The coffee tasted great and was at such a good price! I highly recommend this to everyone!\n","\n","\n","- Summary:\n"," great coffee great\n","\n","\n","\n","- Review:\n"," love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets\n","\n","\n","- Summary:\n"," i love\n","\n","\n"],"name":"stdout"}]},{"metadata":{"id":"avX-3LbkmFs4","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}